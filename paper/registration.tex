
\section{point cloud registration}
\label{sec:registration}


%We achieve the depth data from the 2 NIR cameras using depth estimation methods~\cite{Bleyer2011PatchMatch}.
After the global optimization, we get a set of camera parameters with minimal reprojection error.
However, there are still many problems in the 3D model obtained from directly merging the partial point clouds estimated from different views according to the estimated camera poses, mainly due to the low quality of depth estimation.
%
%If the depth and camera parameters are both accurate enough, the point clouds of different views should align very well.
Most depth-estimation techniques based on NIR cameras are not robust enough, especially for light-absorbing materials like black hair.
Moreover, the depth estimation may be affected by many factors like the quality of the laser pointer, the interaction effect on the camera pods which are arranged towards each other caused by the laser and similarity.
%Although there have been many research on it, the error cannot be avoided completely.
As Fig.~\ref{fig:deptherror} shows, the estimated depth encounters large distortions on the boundary of the human body, and missing data in the head area.
Directly merge the point clouds estimated from 8 views according to the estimated camera parameters leads to a noisy and distorted point cloud, as Fig.~\ref{fig:} (a)(b) shows.\md{ToDo}

\begin{figure*}[ht]
  \centering
\subfigure[]{
\begin{minipage}[c]{.22\linewidth}
\centering
  \includegraphics[width=4cm]{image/depth_error_rgb.png}
\end{minipage}
}%
\subfigure[]{
\begin{minipage}[c]{.22\linewidth}
\centering
  \includegraphics[width=4cm]{image/depth_error_depth.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[c]{.22\linewidth}
\centering
  \includegraphics[width=4cm]{image/depth_error_pc.png}
\end{minipage}
}
\subfigure[]{
\begin{minipage}[c]{.22\linewidth}
\centering
  \includegraphics[width=4cm]{image/depth_error_pc_2.png}
\end{minipage}
}
\caption{(a): RGB data. (b): Depth data. (c): The point cloud reconstruted by (a) and (b). (d): (c) in another view. The distortion near the head of the model of a single view which is caused by the inaccurate depth estimation can seen in the red box.}
\label{fig:deptherror}
\end{figure*}


The point clouds reconstructed from different views are quite near to each other because of the global calibration, hence there is no need for the coarse registration. We tried the state-of-the-art global registration algorithms~\cite{Evangelidis-ECCV-2014} to register all the point clouds from different views simultaneously without the corresponding points. However, the point clouds produced by our multi-camera system may have no overlapping fields, which leads to the wrong results. For example, the point cloud of the back of a human body may be aligned to the front overlappingly because of the symmetry of the shape of the body. To avoid these kinds of mistakes, we register the point clouds in multi views sequentially.

We choose the point cloud of one view as the reference, and align the point cloud of its neighboring views sequentially, using the Iterative Closest Point algorithm. \xj{Add the reference. If do not considering the time, why don't you use other state-of-the-art algorithms?}
The registration step is to minimize the distance between the corresponding points in different views mainly caused by the depth maps, which can be defined as:
\begin{equation} 
E(\mathbf{R},\mathbf{T})=\sum_{N}(\mathbf{P}_{t}-\mathbf{R}\times\mathbf{P}_{s}-\mathbf{T})^{2},
\end{equation}
where $\mathbf{P}_{t}$ and $\mathbf{P}_{s}$ are a pair of corresponding points in the point clouds of the two views, N is the total number of the corresponding points, $\mathbf{R}$ and $\mathbf{T}$ represents the rigid transform, $\mathbf{R}$ is the rotation matrix and $\mathbf{T}$ is the translation matrix.

After the registration step, the point cloud which are not aligned very well because of the depth maps can be registered and a high-quality 3D model can be achieved.

\comments{
Moreover, although the reprojection error after the global optimization mentioned in the last section is at sub-pixel level, which can prove the high accuracy of our calibration result, we still find the separation between the point cloud of different views when we map the depth to the entire model, effecting the quality of reconstruction. These deficiencies are mainly caused by the depth estimation.




To achieve a high-quality reconstruction, we fuse partial point clouds which are inaccurate and incomplete into a 3D model in a registration manner.
%
For each point reconstructed by the depth image, we consider an estimation.
\begin{equation}
\mathbf{\tilde{P}}_{ij}=f_{i}(\mathbf{P}_{j}),
\end{equation}
where $\mathbf{P}_{j}$ is the true 3D coordinates in the camera coordinate system. $f_{i}$ is a nonlinear function for view $i$, represent the depth influence. $\mathbf{\tilde{P}}_{ij}$ is the 3D coordinates we get from the depth data and intrinsic parameters in view $i$. With the extrinsic parameter, we can transform all the points into the world coordinate system. The estimation can be written as
\begin{equation}
\mathbf{\hat{P}}_{gj}=g(\mathbf{K_{ex}}_{i})f_{i}(\mathbf{P}_{j}),
\end{equation}
where $\mathbf{\hat{P}}_{gj}$ is the 3D coordinates in the world coordinate system we reconstruct from the depth and camera parameters. ICP can be replaced as a rigid transform to all views
\begin{equation}
\mathbf{\tilde{\hat{P}}}_{gj}=M_{i}g(\mathbf{K_{ex}}_{i})f_{i}(\mathbf{P}_{j})=g(\mathbf{K_{ex}}_{i})M_{i}f_{i}(\mathbf{P}_{j}),
\end{equation}
where $\mathbf{\tilde{\hat{P}}}_{gj}$ is the coordinates of the point after the alignment. The transform $M_{i}$ can refine the error caused by $f_{i}$, and improve the quality of the result of 3D reconstruction.
}%

% view 2 to it, combine the result of the two views, then align the point cloud of view 3 to the combined result and so on.
%Each ICP process produce a transformation matrix, then we can map the inaccurate depth to a high-quality 3D model using the rigid transformation.


